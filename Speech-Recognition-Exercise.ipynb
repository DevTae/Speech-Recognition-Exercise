{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강의 url : https://youtube.com/playlist?list=PL9mhQYIlKEhdrYpsGk8X4qj3tQUuaDhrl\n",
    "# Colab 원본 : https://colab.research.google.com/drive/1wMRLEK0cwToY0yCMW4K8kkP2dHS5KOtq?usp=sharing#scrollTo=TpycP9ofzGek\n",
    "# 참조 논문 : Deep Speech to, Listen attend and spell\n",
    "# 딥러닝 기초 리뷰 : FC Layer, CNN, RNN, LSTM, Attention\n",
    "# Audio Classfication & Tagging : 데이터 파이프라인의 이해\n",
    "# CTC : 논문의 이해, CTC Loss 쓰는 법, Deep Speech2 구현\n",
    "# LAS : Extra 모델 아키텍쳐 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43954060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram 을 input 으로 받고, invariant convolution 을 거친 다음에, RNN (Recurrent Neural Network) 을 거치고, Fully Connected 를 거친다.\n",
    "# 마지막으로 나온 CTC(Connectionist Temporal Classification) 를 Decode 하여 결과값을 얻어냄.\n",
    "# IO Module, Network Architecture, Loss 함수만 잘 설정하면 좋은 모델 만들 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connectivity Patterns + Nonlinearity modules\n",
    "\n",
    "# Connectivity Patterns\n",
    "# - Fully-Connected (FC Layer)\n",
    "# - Convolutional\n",
    "# - Dilated\n",
    "# - Recurrent\n",
    "# - Skip / Residual\n",
    "# - Random\n",
    "\n",
    "# Nonlinearity modules\n",
    "# - ReLU\n",
    "# - Sigmoid\n",
    "# - Tanh\n",
    "# - GRU\n",
    "# - LSTM\n",
    "\n",
    "# Loss\n",
    "# - Cross Entropy\n",
    "# - Adversarial\n",
    "# - Variational\n",
    "# - Maximum Likelihood\n",
    "# - L1 and L2\n",
    "\n",
    "# Optimizer\n",
    "# - SGD\n",
    "# - Momentum\n",
    "# - RMSProp\n",
    "# - Adagrad\n",
    "# - Adam\n",
    "# - Second Order\n",
    "\n",
    "# Hyper Parameters\n",
    "# - Learning rate\n",
    "# - Weight decay\n",
    "# - Layer size\n",
    "# - Batch size\n",
    "# - Dropout rate\n",
    "# - Weight initialization\n",
    "# - Data augmentation\n",
    "# - Gradient clipping\n",
    "# - Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully-Connected (FC Layer) 는 Multi-Layer Perceptron 에 Non-Linearity module 만 뺀 것\n",
    "# x 에 대해 모든 입력값을 가중치 W 입력을 통해 z1, activation 함수를 통해 h, 또 다른 activation 함수로 z2 (y)\n",
    "# h 를 만드는 Non-Linearity Function 으로는 Sigmoid, Tanh, ReLU, Leaky ReLU 가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87722d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Layer 는 입력을 필터로 사용하여 Convolution 연산 진행\n",
    "# 하이퍼 파라미터에는 필터 크기 F 및 보폭 S 포함. 결과 출력 O 를 feature map, activation map 이라고 부름\n",
    "# Pooling Layer 는 다운 샘플링 작업으로, 일반적으로 Convolution Layer 이후 적용, spatial invariance (공간에 영향 없는) 를 수행.\n",
    "# 특히, Max pool 과 Average Pool 는 각각 최대 값과 평균값을 취하는 특수 종류의 풀링\n",
    "# Pooling 된 데이터를 Fully Connected 로 연결시킬 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe36c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D CNN in Spectrogram\n",
    "# Convolution filter 의 크기가 frequency 영역대는 고정되어 있으며, Time 에 따라 진행\n",
    "\n",
    "# 2-D CNN in Spectrogram\n",
    "# Convolution filter 의 크기가 frequency과 Time에 따라서 진행됩니다.\n",
    "\n",
    "# Sample CNN\n",
    "# 바로 input 데이터를 waveform 그 자체로 사용할 수 있다.\n",
    "# CNN 이 \"phase-invariant\" representation 을 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 은 Hidden State 를 유지하면서 이전 출력을 입력으로 사용할 수 있는 신경망임.\n",
    "# a^<T> = tanh(WaxX^<t> + WaaA^<t-1> + ba)\n",
    "# y^<T> = softmax(WyaA^<t> + by)\n",
    "\n",
    "# LSTM\n",
    "# GRU (Gated Recurrent Unit) 및 LSTM (Long Short-Term Memory Unit)은 기존 RNN에서 vanishing gradient 를 처리하며 LSTM은 GRU의 일반화된 모델임.\n",
    "# Forget 게이트 등 여러 게이트를 바탕으로 옛 데이터를 기억하도록 함 (RNN 개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "# Encoding 된 정보와 Decoding 된 정보의 Hidden State 간의 Alignment 를 맞추는 것\n",
    "# Hidden State 를 맞추다보면 어떤 스텝에서 어떤 Utterance 를 나타내는지 알 수 있음.\n",
    "# 어떤 시점에 대한 인식과 번역에 쓰임.\n",
    "\n",
    "# RNN계열 encoding 방식에서는 계속 마지막 hidden state까지 학습을 하면서 연산을 해야 했다.\n",
    "# 이러한 문제를 해결하기 위해 input source 와 hidden state 의 관계를 학습시키는 추가적인 Network 를 만든 Attention 이 나옴.\n",
    "# 이 attention 은 output 에 의하여 weight 를 학습하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07ce0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Layer\n",
    "\n",
    "layer_dims = [5,4,3,1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    # dictionary 객체 생성\n",
    "    parameters = {}\n",
    "    # 총 layer들의 길이를 계산\n",
    "    L = len(layer_dims)\n",
    "    # 레이어들을 돌면서, 레이어들 간의 weight와 bias의 초기값의 난수 생성\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # assert를 통해, dimension을 맞추줍니다. 틀릴시 error 발생\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d066f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.96611671, -0.29382973, -0.39793937, -0.39615487, -0.01055339],\n",
       "        [-0.48097877, -0.03133202,  0.68878007,  0.25151511,  0.16290848],\n",
       "        [ 0.15159429, -0.15188782, -0.31257492,  0.50734366, -0.54890941],\n",
       "        [ 0.13220054, -0.41171213,  0.3657522 , -0.05180709, -0.36550746]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.37020287,  0.00858161, -0.65042959,  0.58061724],\n",
       "        [-0.07171643, -0.09504507,  0.11313475, -0.58871746],\n",
       "        [-0.17561212,  0.01274369, -0.07963832, -0.18083417]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[ 1.19715607, -0.1820765 ,  0.29266063]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep(layer_dims)\n",
    "parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a350b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    # W에 A를 내적하게 됩니다. 그후에는 b를 더해줍니다.\n",
    "    Z = W.dot(A) + b\n",
    "    # Z의 shape이 input과 weight의 shape과 동일한지를 체크합니다.\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    # 계산단계에서 사용한 값을 cache에 저장해둡니다.\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    # Activation function의 종류에 따라서 값을 나누어 줍니다.\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    # Shape이 input과 weight와 동일한지 체크해줍니다.\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    # linear 연산과 activation 연산을 cache에 저장해둡니다.\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28884e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    # cache 들의 list입니다.\n",
    "    caches = []\n",
    "    A = X\n",
    "    # weight와 bias가 저장되어 있기 때문에 //2 를 해주어야 layer의 사이즈가 됩니다.\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # hidden layersms relu를 통과\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # output layer는 sigmoid를 통과하게 한다\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb63c66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50101001, 0.49254327, 0.60241226, 0.5029799 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randn(5,4)\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c825522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function\n",
    "# Arguments - AL : 뉴럴넷 통과하여 나온 y, Y : 실제 label vector\n",
    "# Returns - cost : cross-entropy cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70173f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1.0/m)*np.sum(np.multiply(Y,np.log(AL)) + np.multiply(1-Y, np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a3bc5dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.6523200689754123\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(AL, Y)\n",
    "print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c3257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation process\n",
    "# LINEAR backward\n",
    "# LINEAR -> ACTIVATION backward\n",
    "# Layer -> Layer backward\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ,cache[0].T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db =  linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4db961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {} # 빈 dictionary 호출\n",
    "    L = len(caches) # 레이어의 갯수를 caches로 부터 받아옵니다.\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # Shape을 AL과 동일하게 해줍니다.\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y,AL)- np.divide(1-Y, 1-AL))\n",
    "    # caches index를 잡아둡니다.\n",
    "    current_cache = caches[L-1] \n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # indexing입니다.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d8b11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA2': array([[ 0.59978718, -0.60750491, -0.47597458,  0.60214545],\n",
       "        [-0.09122215,  0.09239594,  0.07239138, -0.09158082],\n",
       "        [ 0.1466259 , -0.14851261, -0.11635828,  0.14720241]]),\n",
       " 'dW3': array([[-0.03315558, -0.02078383,  0.00135047]]),\n",
       " 'db3': array([[0.02473636]]),\n",
       " 'dA1': array([[ 0.19629365, -0.00662631, -0.17620715,  0.19706544],\n",
       "        [ 0.0070157 , -0.00878178, -0.00408463,  0.00704328],\n",
       "        [-0.40179637,  0.01045319,  0.30958795, -0.40337617],\n",
       "        [ 0.3217318 , -0.0543951 , -0.27635905,  0.3229968 ]]),\n",
       " 'dW2': array([[-0.0551107 ,  0.14156149,  0.        , -0.03531602],\n",
       "        [ 0.        ,  0.0061461 ,  0.04558699,  0.00134032],\n",
       "        [ 0.00026072,  0.03460658,  0.        ,  0.        ]]),\n",
       " 'db2': array([[0.18148951],\n",
       "        [0.02309899],\n",
       "        [0.07345708]]),\n",
       " 'dA0': array([[-0.0033744 , -0.00138257, -0.20677149,  0.18700055],\n",
       "        [-0.00021982,  0.02108256,  0.16555527, -0.05812437],\n",
       "        [ 0.00483227, -0.02921125, -0.03095917, -0.07356883],\n",
       "        [ 0.00176455,  0.00591266,  0.08412268, -0.07629694],\n",
       "        [ 0.00114292,  0.01271333,  0.10287088, -0.0009323 ]]),\n",
       " 'dW1': array([[ 0.01331132,  0.04572519,  0.0306372 ,  0.01676787,  0.09839202],\n",
       "        [ 0.00042407,  0.00558553,  0.00352354, -0.00831828,  0.00684977],\n",
       "        [-0.00150476, -0.00314506, -0.00253932,  0.00707085, -0.00095974],\n",
       "        [ 0.01761371,  0.08934042,  0.01434525,  0.01490381, -0.02376189]]),\n",
       " 'db1': array([[ 0.00521457],\n",
       "        [ 0.0013193 ],\n",
       "        [ 0.0026133 ],\n",
       "        [-0.08268854]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = L_model_backward(AL, Y, caches)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27f1240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # 레이어의 갯수입니다.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddcb6e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.96545114, -0.29611599, -0.39947123, -0.39699326, -0.01547299],\n",
       "        [-0.48099997, -0.0316113 ,  0.6886039 ,  0.25193102,  0.16256599],\n",
       "        [ 0.15166953, -0.15173056, -0.31244795,  0.50699012, -0.54886142],\n",
       "        [ 0.13131985, -0.41617915,  0.36503494, -0.05255228, -0.36431937]]),\n",
       " 'b1': array([[-2.60728589e-04],\n",
       "        [-6.59649945e-05],\n",
       "        [-1.30664895e-04],\n",
       "        [ 4.13442692e-03]]),\n",
       " 'W2': array([[ 0.3729584 ,  0.00150354, -0.65042959,  0.58238304],\n",
       "        [-0.07171643, -0.09535238,  0.1108554 , -0.58878447],\n",
       "        [-0.17562516,  0.01101337, -0.07963832, -0.18083417]]),\n",
       " 'b2': array([[-0.00907448],\n",
       "        [-0.00115495],\n",
       "        [-0.00367285]]),\n",
       " 'W3': array([[ 1.19881385, -0.1810373 ,  0.2925931 ]]),\n",
       " 'b3': array([[-0.00123682]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = update_parameters(parameters, grads, 0.05)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2d69cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 3, 2) (4, 7, 7, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-0.01296614,  0.00608499],\n",
       "         [ 0.90796727,  0.05052092],\n",
       "         [-0.18234052,  0.51504166],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 1.28240989,  0.84210731],\n",
       "         [ 0.35115932,  0.17818944],\n",
       "         [-0.85912262, -0.68244724],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-0.09496185,  0.25291029],\n",
       "         [ 0.78523552, -0.29575214],\n",
       "         [ 0.02715236, -2.11410721],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-0.31053679, -0.92108202],\n",
       "         [ 0.24891156,  0.3923755 ],\n",
       "         [-0.80378672,  0.6316999 ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-1.00720974, -2.08591213],\n",
       "         [-0.97054674,  0.60438436],\n",
       "         [-1.54787195,  0.71757754],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-0.98028888,  1.97523091],\n",
       "         [-0.4208488 , -0.36889317],\n",
       "         [-0.21172653, -0.10478547],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.93998106,  0.12012127],\n",
       "         [-1.13864037, -0.98369991],\n",
       "         [-0.43809583,  1.45335002],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-0.52371767, -0.42170614],\n",
       "         [-0.90099743,  0.88149231],\n",
       "         [-0.24525776, -0.50048177],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.43095321, -0.57041611],\n",
       "         [-0.36356226, -1.33091537],\n",
       "         [ 0.18451889, -0.63613605],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-0.41321667, -0.65793806],\n",
       "         [-0.1434713 ,  0.62577314],\n",
       "         [-2.45852432, -1.1156623 ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.00540554,  0.66573331],\n",
       "         [ 0.5784767 ,  0.55530255],\n",
       "         [-0.5926868 , -0.70075445],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [-0.53614366,  1.82872852],\n",
       "         [-0.43238201, -1.29688624],\n",
       "         [-0.30399248,  1.04620854],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNN : filter, pooling \n",
    "def zero_pad(X,pad):\n",
    "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)),'constant',constant_values=0)\n",
    "    return X_pad\n",
    "\n",
    "x = np.random.randn(4,3,3,2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print(x.shape, x_pad.shape)\n",
    "x_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73b21406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    # Element-wise product\n",
    "    s = a_slice_prev * W\n",
    "    # 채널을 기반으로 모두 더해줍니다.\n",
    "    Z = np.sum(s)\n",
    "    # Bias b를 더해줍니다.\n",
    "    Z = Z + np.float(b)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c04b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    # Input의 shpae을 정의합니다.\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    # filter의 shape을 정의합니다.\n",
    "    (f,f,n_C_prev,n_C) = W.shape\n",
    "    # input dictionary에서 받을 value 값입니다.\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Conv의 output volumn을 정의해줍니다\n",
    "    n_H = int(((n_H_prev - f + (2*pad)) / stride)+1)\n",
    "    n_W = int(((n_W_prev - f + (2*pad)) / stride)+1)\n",
    "    \n",
    "    # output volumn을 initialize해줍시다\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Padding을 설정해줍니다.\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "    for i in range(m): #batch에 있는 traindata를 조회\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        for h in range(n_H): #hight를 돌고\n",
    "            for w in range(n_W): #width를 돌고\n",
    "                for c in range(n_C): #Channel을 돌면서\n",
    "                    #input의 slice를 해줍시다\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]\n",
    "                    Z[i,h,w,c]=conv_single_step(a_slice_prev,W[...,c],b[...,c])\n",
    "    assert(Z.shape == (m,n_H,n_W,n_C))\n",
    "    cache = (A_prev,W,b,hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72d4c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.048995203528855794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devtae\\AppData\\Local\\Temp/ipykernel_3880/664427259.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Z = Z + np.float(b)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "print(\"Z's mean =\", np.mean(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7d45d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
    "    # input의 shape을 받아옵니다\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    # filter size와 stride size를 받아옵니다.\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    # ouput dimension을 잡아줍시다\n",
    "    n_H = int(1+(n_H_prev-f)/stride)\n",
    "    n_W = int(1+(n_W_prev-f)/stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i,h,w,c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i,h,w,c] = np.mean(a_prev_slice)\n",
    "                        \n",
    "    cache = (A_prev, hparameters)\n",
    "    assert(A.shape ==(m,n_H,n_W,n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28b5ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.74481176 0.86540763 1.13376944]]]\n",
      "\n",
      "\n",
      " [[[1.13162939 1.51981682 2.18557541]]]]\n",
      "[[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
      "\n",
      "\n",
      " [[[-0.22154621  0.51716526  0.48155844]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(A)\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
